{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import layers, models\n",
    "from keras.layers import Layer, Embedding, Dense, Dropout, LayerNormalization\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from MIDI_data_extractor import MIDI_data_extractor\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_padding_mask(seq, n=4):\n",
    "    mask = tf.cast(seq == -1, tf.float32)\n",
    "    return tf.reshape(mask, (tf.shape(mask)[0], *(1,) * (n - 2), tf.shape(mask)[-1]))\n",
    "\n",
    "\n",
    "def create_look_ahead_mask(seq_len):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "    return tf.cast(mask, tf.float32)\n",
    "\n",
    "\n",
    "def create_mask(inp, n=4):\n",
    "    padding_mask = create_padding_mask(inp, n)\n",
    "    seq_len = tf.shape(inp)[1]  # Get the sequence length dynamically\n",
    "    look_ahead_mask = create_look_ahead_mask(seq_len)\n",
    "    return tf.maximum(padding_mask, look_ahead_mask)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def positional_encoding(position, d_model):\n",
    "    def get_angles(pos, i, d_model):\n",
    "        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "        return pos * angle_rates\n",
    "\n",
    "    angle_rads = get_angles(\n",
    "        np.arange(position)[:, np.newaxis],\n",
    "        np.arange(d_model)[np.newaxis, :],\n",
    "        d_model,\n",
    "    )\n",
    "\n",
    "    # apply sin to even indices in the array; 2i\n",
    "    sines = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    cosines = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = np.concatenate([sines, cosines], axis=-1)\n",
    "\n",
    "    return tf.cast(pos_encoding[np.newaxis, ...], dtype=tf.float32)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def skew(tensor):\n",
    "\n",
    "  paddings = [[0, 0] for _ in range(len(tensor.shape) - 1)]\n",
    "  padded = tf.pad(tensor, [*paddings, [1, 0]])\n",
    "\n",
    "  Srel = tf.reshape(padded, (-1, tensor.shape[-1] + 1, tensor.shape[-2]))[:, 1:]\n",
    "  return tf.cast(tf.reshape(Srel, tensor.shape), tensor.dtype)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class PositionWiseFeedForwardNetwork(layers.Layer):\n",
    "    def __init__(self, embed_dim, ffn_dim):\n",
    "        super(PositionWiseFeedForwardNetwork, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.ffn_dim = ffn_dim\n",
    "\n",
    "        self.dense1 = layers.Dense(ffn_dim, activation=\"relu\")\n",
    "        self.dense2 = layers.Dense(embed_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dense2(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create a class that applies relative positional encoding\n",
    "class RelativePositionalEncoding(layers.Layer):\n",
    "    def __init__(self, embed_dim, max_seq_len):\n",
    "        super(RelativePositionalEncoding, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.pos_encoding = self.add_weight(\n",
    "            shape=(self.max_seq_len, self.embed_dim),\n",
    "            initializer=\"he_intializer\",\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs + self.pos_encoding\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "class MultiHeadAttention(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.depth = embed_dim // num_heads\n",
    "\n",
    "        self.wq = layers.Dense(embed_dim)\n",
    "        self.wk = layers.Dense(embed_dim)\n",
    "        self.wv = layers.Dense(embed_dim)\n",
    "\n",
    "        self.dense = layers.Dense(embed_dim)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        q, k, v, mask = inputs\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "\n",
    "        # apply relative positional encoding\n",
    "        q = RelativePositionalEncoding(self.embed_dim, q.shape[2])(q)\n",
    "        k = RelativePositionalEncoding(self.embed_dim, k.shape[2])(k)\n",
    "\n",
    "        # apply skew function\n",
    "        k = skew(k)\n",
    "\n",
    "        # calculate attention\n",
    "        matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "        # apply mask\n",
    "        if mask is not None:\n",
    "            scaled_attention_logits += mask * -1e9\n",
    "\n",
    "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "        attention = tf.matmul(attention_weights, v)\n",
    "\n",
    "        # concat attention\n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim))\n",
    "\n",
    "        # apply dense layer\n",
    "        output = self.dense(concat_attention)\n",
    "        return output, attention_weights"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "class DecoderBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ffn_dim, dropout_rate=0.1):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.ffn_dim = ffn_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.att = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.ffn = PositionWiseFeedForwardNetwork(embed_dim, ffn_dim)\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, encoding = inputs\n",
    "        # create a mask for the decoder inputs\n",
    "        causal_mask = tf.linalg.band_part(tf.ones((tf.shape(x)[1], tf.shape(x)[1])), -1, 0)\n",
    "        causal_mask = tf.reshape(causal_mask, (1, causal_mask.shape[0], causal_mask.shape[1]))\n",
    "        causal_mask = tf.cast(causal_mask, tf.bool)\n",
    "        # create a mask for the encoder inputs\n",
    "        padding_mask = tf.math.equal(x, 0)\n",
    "        padding_mask = tf.reshape(padding_mask, (tf.shape(x)[0], 1, 1, tf.shape(x)[1]))\n",
    "        padding_mask = tf.cast(padding_mask, tf.bool)\n",
    "        # create a mask for the encoder inputs\n",
    "        combined_mask = tf.maximum(causal_mask, padding_mask)\n",
    "        # apply multi-head attention\n",
    "        x = self.att([x, x, x, combined_mask])\n",
    "        x = self.dropout1(x)\n",
    "        x = self.layernorm1(x + encoding)\n",
    "        # apply feed forward network\n",
    "        x = self.ffn(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.layernorm2(x + encoding)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Create the decoder class that uses relative positional encoding and also allows for a matrix input of x by 16 size and a matrix output of 16 different outputs with softmax sizes of: [129,129,129,129,129,2,200,32,32,128,64,30,3000,129,5,129]\n",
    "class Decoder(layers.Layer):\n",
    "    def __init__(self, num_layers, embed_dim, num_heads, ffn_dim, vocab_size, max_seq_len, dropout_rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.ffn_dim = ffn_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.embedding = layers.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_embedding = RelativePositionalEncoding(embed_dim, max_seq_len)\n",
    "        self.dropout = layers.Dropout(dropout_rate)\n",
    "        self.decoder_blocks = [DecoderBlock(embed_dim, num_heads, ffn_dim, dropout_rate) for _ in range(num_layers)]\n",
    "        self.final_layer = layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, encoding = inputs\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "        # create embeddings\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.embed_dim, tf.float32))\n",
    "        x = self.pos_embedding(x)\n",
    "        x = self.dropout(x)\n",
    "        # apply decoder blocks\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.decoder_blocks[i]([x, encoding])\n",
    "        # final dense layer\n",
    "        x = self.final_layer(x)\n",
    "        return x, attention_weights"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create the Final Model that has:\n",
    "# inputs: x by 16  array\n",
    "# outputs: 16 different outputs with softmax sizes of: [129,129,129,129,129,2,200,32,32,128,64,30,3000,129,5,129]\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create custom loss function\n",
    "class CustomLoss(layers.Layer):\n",
    "    def __init__(self, name=\"custom_loss\"):\n",
    "        super(CustomLoss, self).__init__(name=name)\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(\n",
    "            y_true, y_pred, from_logits=True\n",
    "        )\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        return loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create learning rate schedule\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, embed_dim, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.embed_dim = tf.cast(self.embed_dim, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        return tf.math.rsqrt(self.embed_dim) * tf.math.minimum(arg1, arg2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create training loop\n",
    "def train_model(model, dataset, epochs, learning_rate):\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "    train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
    "    train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "        name=\"train_accuracy\"\n",
    "    )\n",
    "    loss_fn = CustomLoss()\n",
    "    for epoch in range(epochs):\n",
    "        print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "        for step, (x_batch_train, y_batch_train) in enumerate(dataset):\n",
    "            with tf.GradientTape() as tape:\n",
    "                logits = model(x_batch_train, training=True)\n",
    "                loss = loss_fn(y_batch_train, logits)\n",
    "            gradients = tape.gradient(loss, model.trainable_weights)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
    "            train_loss(loss)\n",
    "            train_accuracy(y_batch_train, logits)\n",
    "            if step % 50 == 0:\n",
    "                print(\n",
    "                    \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                    % (step, float(train_loss.result()))\n",
    "                )\n",
    "                print(\n",
    "                    \"Training accuracy (for one batch) at step %d: %.4f\"\n",
    "                    % (step, float(train_accuracy.result()))\n",
    "                )\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create model\n",
    "def create_model(num_layers, embed_dim, num_heads, ffn_dim, vocab_size, max_seq_len, dropout_rate):\n",
    "    inputs = layers.Input(shape=(None,), dtype=tf.int64)\n",
    "    encoding = layers.Input(shape=(None, None))\n",
    "    x = Decoder(num_layers, embed_dim, num_heads, ffn_dim, vocab_size, max_seq_len, dropout_rate)([inputs, encoding])\n",
    "    return tf.keras.Model(inputs=[inputs, encoding], outputs=x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# initialize parameters\n",
    "num_layers = 2\n",
    "embed_dim = 128\n",
    "num_heads = 8\n",
    "ffn_dim = 512\n",
    "vocab_size = 27374889555\n",
    "dropout_rate = 0.1\n",
    "max_seq_len = 16\n",
    "learning_rate = CustomSchedule(embed_dim)\n",
    "epochs = 10"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_token_by_token_dataset(data, window_size):\n",
    "    num_rows, num_cols = data.shape\n",
    "\n",
    "    training_examples = []\n",
    "    for i in range(num_rows - window_size):\n",
    "        input_seq = data[i:i + window_size]\n",
    "        target_token = data[i + window_size]\n",
    "        training_examples.append((input_seq, target_token))\n",
    "\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(training_examples)\n",
    "    train_dataset = train_dataset.shuffle(buffer_size=len(training_examples))\n",
    "    train_dataset = train_dataset.batch(batch_size=32)\n",
    "\n",
    "    return train_dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = MIDI_data_extractor('chaconne.mid')\n",
    "dataset = create_token_by_token_dataset(data, 16)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# begin training\n",
    "model = create_model(num_layers, embed_dim, num_heads, ffn_dim, vocab_size, max_seq_len, dropout_rate)\n",
    "#create numpy randomized dataset (2D arrays that have 16 long rows)for training\n",
    "\n",
    "train_model(model, dataset, epochs, learning_rate)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
