{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-07-16T18:03:00.066859Z",
     "start_time": "2023-07-16T18:03:00.042668200Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import layers, models\n",
    "from keras.layers import Embedding, Dropout, LayerNormalization\n",
    "\n",
    "from MIDI_data_extractor import MIDI_data_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "outputs": [],
   "source": [
    "def create_padding_mask(seq, n=4):\n",
    "    mask = tf.cast(seq == -1, tf.float32)\n",
    "    return tf.reshape(mask, (tf.shape(mask)[0], *(1,) * (n-2), tf.shape(mask)[-1]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-16T18:03:00.090697500Z",
     "start_time": "2023-07-16T18:03:00.058696900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(seq_len):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "    return tf.cast(mask, tf.float32)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-16T18:03:00.098695700Z",
     "start_time": "2023-07-16T18:03:00.066859Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "outputs": [],
   "source": [
    "def create_mask(inp, n=4):\n",
    "    padding_mask = create_padding_mask(inp, n)\n",
    "    seq_len = tf.shape(inp)[1]  # Get the sequence length dynamically\n",
    "    look_ahead_mask = create_look_ahead_mask(seq_len)\n",
    "    return tf.maximum(padding_mask, look_ahead_mask)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-16T18:03:00.106695700Z",
     "start_time": "2023-07-16T18:03:00.082769Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "outputs": [],
   "source": [
    "class RelativePositionMultiHeadAttention(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(RelativePositionMultiHeadAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.depth = embed_dim // num_heads\n",
    "\n",
    "        self.query_dense = layers.Dense(embed_dim)\n",
    "        self.key_dense = layers.Dense(embed_dim)\n",
    "        self.value_dense = layers.Dense(embed_dim)\n",
    "        self.relative_embeddings = self.add_weight(\"relative_embeddings\", shape=[embed_dim, embed_dim])\n",
    "\n",
    "        self.final_dense = layers.Dense(embed_dim)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "\n",
    "    def scaled_dot_product_attention(self, query, key, value, mask):\n",
    "        matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "        # compute relative position embeddings\n",
    "        rel_embeddings = self.relative_embeddings[:tf.shape(query)[-2], :]\n",
    "        rel_embeddings = tf.pad(rel_embeddings, [[tf.shape(query)[-2], 0], [0, 0]])\n",
    "\n",
    "        # reshaping the relative embeddings to match the query shape\n",
    "        rel_embeddings = tf.reshape(rel_embeddings, [1, 1, tf.shape(query)[-2], tf.shape(query)[-2], self.num_heads, self.depth])\n",
    "        rel_embeddings = tf.tile(rel_embeddings, [tf.shape(query)[0], tf.shape(query)[1], 1, 1, 1, 1])\n",
    "\n",
    "        matmul_qk += rel_embeddings\n",
    "        # scale matmul_qk\n",
    "        dk = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "        # add the mask to the scaled tensor.\n",
    "        if mask is not None:\n",
    "            scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "        # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "        # add up to 1.\n",
    "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-3)  # Updated axis here\n",
    "        output = tf.matmul(attention_weights, value)\n",
    "        return output, attention_weights\n",
    "\n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.query_dense(q)\n",
    "        k = self.key_dense(k)\n",
    "        v = self.value_dense(v)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "\n",
    "        # Use the mask\n",
    "        scaled_attention, attention_weights = self.scaled_dot_product_attention(q, k, v, mask[:, tf.newaxis, :, :])\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.embed_dim))\n",
    "        output = self.final_dense(concat_attention)\n",
    "\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-16T18:03:00.130061800Z",
     "start_time": "2023-07-16T18:03:00.120843Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "outputs": [],
   "source": [
    "class PositionWiseFeedForwardNetwork(layers.Layer):\n",
    "    def __init__(self, embed_dim, ffn_dim):\n",
    "        super(PositionWiseFeedForwardNetwork, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.ffn_dim = ffn_dim\n",
    "\n",
    "        self.dense1 = layers.Dense(ffn_dim, activation=\"relu\")\n",
    "        self.dense2 = layers.Dense(embed_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dense2(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-16T18:03:00.146594400Z",
     "start_time": "2023-07-16T18:03:00.137574300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ffn_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = RelativePositionMultiHeadAttention(embed_dim, num_heads)\n",
    "        self.ffn = PositionWiseFeedForwardNetwork(embed_dim, ffn_dim)\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training, mask):\n",
    "        attn_output = self.att(inputs, inputs, inputs, mask)\n",
    "        print(f\"Shape of attn_output: {attn_output.shape}\")\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-16T18:03:00.161874100Z",
     "start_time": "2023-07-16T18:03:00.153604700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"transformer_block_37\" (type TransformerBlock).\n\nin user code:\n\n    File \"C:\\Users\\ilove\\AppData\\Local\\Temp\\ipykernel_2140\\3159178080.py\", line 12, in call  *\n        attn_output = self.att(inputs, inputs, inputs, mask)\n    File \"C:\\Users\\ilove\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\ilove\\AppData\\Local\\Temp\\__autograph_generated_filegmkrvd7h.py\", line 18, in tf__call\n        scaled_attention = ag__.converted_call(ag__.ld(tf).transpose, (ag__.ld(scaled_attention),), dict(perm=[0, 2, 1, 3]), fscope)\n\n    ValueError: Exception encountered when calling layer 'relative_position_multi_head_attention_37' (type RelativePositionMultiHeadAttention).\n    \n    in user code:\n    \n        File \"C:\\Users\\ilove\\AppData\\Local\\Temp\\ipykernel_2140\\3293392212.py\", line 59, in call  *\n            scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n    \n        ValueError: Dimension must be 6 but is 4 for '{{node transformer_block_37/relative_position_multi_head_attention_37/transpose_5}} = Transpose[T=DT_FLOAT, Tperm=DT_INT32](transformer_block_37/relative_position_multi_head_attention_37/MatMul_1, transformer_block_37/relative_position_multi_head_attention_37/transpose_5/perm)' with input shapes: [?,4,?,4,4,16], [4].\n    \n    \n    Call arguments received by layer 'relative_position_multi_head_attention_37' (type RelativePositionMultiHeadAttention):\n      • v=tf.Tensor(shape=(None, None, 16, 64), dtype=float32)\n      • k=tf.Tensor(shape=(None, None, 16, 64), dtype=float32)\n      • q=tf.Tensor(shape=(None, None, 16, 64), dtype=float32)\n      • mask=tf.Tensor(shape=(None, 1, None, 16), dtype=float32)\n\n\nCall arguments received by layer \"transformer_block_37\" (type TransformerBlock):\n  • inputs=tf.Tensor(shape=(None, None, 16, 64), dtype=float32)\n  • training=None\n  • mask=tf.Tensor(shape=(None, 1, None, 16), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[323], line 17\u001B[0m\n\u001B[0;32m     14\u001B[0m output_dims \u001B[38;5;241m=\u001B[39m [\u001B[38;5;241m129\u001B[39m,\u001B[38;5;241m129\u001B[39m,\u001B[38;5;241m129\u001B[39m,\u001B[38;5;241m129\u001B[39m,\u001B[38;5;241m129\u001B[39m,\u001B[38;5;241m2\u001B[39m,\u001B[38;5;241m200\u001B[39m,\u001B[38;5;241m32\u001B[39m,\u001B[38;5;241m32\u001B[39m,\u001B[38;5;241m128\u001B[39m,\u001B[38;5;241m64\u001B[39m,\u001B[38;5;241m30\u001B[39m,\u001B[38;5;241m3000\u001B[39m,\u001B[38;5;241m129\u001B[39m,\u001B[38;5;241m5\u001B[39m,\u001B[38;5;241m129\u001B[39m]\n\u001B[0;32m     16\u001B[0m \u001B[38;5;66;03m# Create the model\u001B[39;00m\n\u001B[1;32m---> 17\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mcreate_multi_output_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_dim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m16\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43membed_dim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m64\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_heads\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mffn_dim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m128\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_blocks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_dims\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_dims\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     18\u001B[0m model\u001B[38;5;241m.\u001B[39mcompile(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124madam\u001B[39m\u001B[38;5;124m'\u001B[39m, [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msparse_categorical_crossentropy\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mlen\u001B[39m(output_dims))\n",
      "Cell \u001B[1;32mIn[323], line 7\u001B[0m, in \u001B[0;36mcreate_multi_output_model\u001B[1;34m(input_dim, embed_dim, num_heads, ffn_dim, num_blocks, output_dims, dropout_rate)\u001B[0m\n\u001B[0;32m      4\u001B[0m x \u001B[38;5;241m=\u001B[39m Embedding(input_dim, embed_dim)(inputs)\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_blocks):\n\u001B[0;32m      6\u001B[0m     \u001B[38;5;66;03m# Use the mask\u001B[39;00m\n\u001B[1;32m----> 7\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[43mTransformerBlock\u001B[49m\u001B[43m(\u001B[49m\u001B[43membed_dim\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_heads\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mffn_dim\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdropout_rate\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtraining\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      9\u001B[0m outputs \u001B[38;5;241m=\u001B[39m [layers\u001B[38;5;241m.\u001B[39mDense(dim, activation\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msoftmax\u001B[39m\u001B[38;5;124m'\u001B[39m)(x) \u001B[38;5;28;01mfor\u001B[39;00m dim \u001B[38;5;129;01min\u001B[39;00m output_dims]\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m models\u001B[38;5;241m.\u001B[39mModel(inputs\u001B[38;5;241m=\u001B[39minputs, outputs\u001B[38;5;241m=\u001B[39moutputs)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[1;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filed244a6ht.py:10\u001B[0m, in \u001B[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001B[1;34m(self, inputs, training, mask)\u001B[0m\n\u001B[0;32m      8\u001B[0m do_return \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m      9\u001B[0m retval_ \u001B[38;5;241m=\u001B[39m ag__\u001B[38;5;241m.\u001B[39mUndefinedReturnValue()\n\u001B[1;32m---> 10\u001B[0m attn_output \u001B[38;5;241m=\u001B[39m \u001B[43mag__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconverted_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43mag__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mld\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43matt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mag__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mld\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mag__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mld\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mag__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mld\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mag__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mld\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmask\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfscope\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     11\u001B[0m ag__\u001B[38;5;241m.\u001B[39mld(\u001B[38;5;28mprint\u001B[39m)(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mShape of attn_output: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mag__\u001B[38;5;241m.\u001B[39mld(attn_output)\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     12\u001B[0m attn_output \u001B[38;5;241m=\u001B[39m ag__\u001B[38;5;241m.\u001B[39mconverted_call(ag__\u001B[38;5;241m.\u001B[39mld(\u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39mdropout1, (ag__\u001B[38;5;241m.\u001B[39mld(attn_output),), \u001B[38;5;28mdict\u001B[39m(training\u001B[38;5;241m=\u001B[39mag__\u001B[38;5;241m.\u001B[39mld(training)), fscope)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filegmkrvd7h.py:18\u001B[0m, in \u001B[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001B[1;34m(self, v, k, q, mask)\u001B[0m\n\u001B[0;32m     16\u001B[0m v \u001B[38;5;241m=\u001B[39m ag__\u001B[38;5;241m.\u001B[39mconverted_call(ag__\u001B[38;5;241m.\u001B[39mld(\u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39msplit_heads, (ag__\u001B[38;5;241m.\u001B[39mld(v), ag__\u001B[38;5;241m.\u001B[39mld(batch_size)), \u001B[38;5;28;01mNone\u001B[39;00m, fscope)\n\u001B[0;32m     17\u001B[0m (scaled_attention, attention_weights) \u001B[38;5;241m=\u001B[39m ag__\u001B[38;5;241m.\u001B[39mconverted_call(ag__\u001B[38;5;241m.\u001B[39mld(\u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39mscaled_dot_product_attention, (ag__\u001B[38;5;241m.\u001B[39mld(q), ag__\u001B[38;5;241m.\u001B[39mld(k), ag__\u001B[38;5;241m.\u001B[39mld(v), ag__\u001B[38;5;241m.\u001B[39mld(mask)[:, ag__\u001B[38;5;241m.\u001B[39mld(tf)\u001B[38;5;241m.\u001B[39mnewaxis, :, :]), \u001B[38;5;28;01mNone\u001B[39;00m, fscope)\n\u001B[1;32m---> 18\u001B[0m scaled_attention \u001B[38;5;241m=\u001B[39m \u001B[43mag__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconverted_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43mag__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mld\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtf\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtranspose\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mag__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mld\u001B[49m\u001B[43m(\u001B[49m\u001B[43mscaled_attention\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mdict\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mperm\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfscope\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     19\u001B[0m concat_attention \u001B[38;5;241m=\u001B[39m ag__\u001B[38;5;241m.\u001B[39mconverted_call(ag__\u001B[38;5;241m.\u001B[39mld(tf)\u001B[38;5;241m.\u001B[39mreshape, (ag__\u001B[38;5;241m.\u001B[39mld(scaled_attention), (ag__\u001B[38;5;241m.\u001B[39mld(batch_size), \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, ag__\u001B[38;5;241m.\u001B[39mld(\u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39membed_dim)), \u001B[38;5;28;01mNone\u001B[39;00m, fscope)\n\u001B[0;32m     20\u001B[0m output \u001B[38;5;241m=\u001B[39m ag__\u001B[38;5;241m.\u001B[39mconverted_call(ag__\u001B[38;5;241m.\u001B[39mld(\u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39mfinal_dense, (ag__\u001B[38;5;241m.\u001B[39mld(concat_attention),), \u001B[38;5;28;01mNone\u001B[39;00m, fscope)\n",
      "\u001B[1;31mValueError\u001B[0m: Exception encountered when calling layer \"transformer_block_37\" (type TransformerBlock).\n\nin user code:\n\n    File \"C:\\Users\\ilove\\AppData\\Local\\Temp\\ipykernel_2140\\3159178080.py\", line 12, in call  *\n        attn_output = self.att(inputs, inputs, inputs, mask)\n    File \"C:\\Users\\ilove\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\ilove\\AppData\\Local\\Temp\\__autograph_generated_filegmkrvd7h.py\", line 18, in tf__call\n        scaled_attention = ag__.converted_call(ag__.ld(tf).transpose, (ag__.ld(scaled_attention),), dict(perm=[0, 2, 1, 3]), fscope)\n\n    ValueError: Exception encountered when calling layer 'relative_position_multi_head_attention_37' (type RelativePositionMultiHeadAttention).\n    \n    in user code:\n    \n        File \"C:\\Users\\ilove\\AppData\\Local\\Temp\\ipykernel_2140\\3293392212.py\", line 59, in call  *\n            scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n    \n        ValueError: Dimension must be 6 but is 4 for '{{node transformer_block_37/relative_position_multi_head_attention_37/transpose_5}} = Transpose[T=DT_FLOAT, Tperm=DT_INT32](transformer_block_37/relative_position_multi_head_attention_37/MatMul_1, transformer_block_37/relative_position_multi_head_attention_37/transpose_5/perm)' with input shapes: [?,4,?,4,4,16], [4].\n    \n    \n    Call arguments received by layer 'relative_position_multi_head_attention_37' (type RelativePositionMultiHeadAttention):\n      • v=tf.Tensor(shape=(None, None, 16, 64), dtype=float32)\n      • k=tf.Tensor(shape=(None, None, 16, 64), dtype=float32)\n      • q=tf.Tensor(shape=(None, None, 16, 64), dtype=float32)\n      • mask=tf.Tensor(shape=(None, 1, None, 16), dtype=float32)\n\n\nCall arguments received by layer \"transformer_block_37\" (type TransformerBlock):\n  • inputs=tf.Tensor(shape=(None, None, 16, 64), dtype=float32)\n  • training=None\n  • mask=tf.Tensor(shape=(None, 1, None, 16), dtype=float32)"
     ]
    }
   ],
   "source": [
    "def create_multi_output_model(input_dim, embed_dim, num_heads, ffn_dim, num_blocks, output_dims, dropout_rate=0.1):\n",
    "    inputs = layers.Input(shape=(None,input_dim))\n",
    "    mask = create_mask(inputs)\n",
    "    x = Embedding(input_dim, embed_dim)(inputs)\n",
    "    for _ in range(num_blocks):\n",
    "        # Use the mask\n",
    "        x = TransformerBlock(embed_dim, num_heads, ffn_dim, dropout_rate)(x, training=None, mask=mask)\n",
    "\n",
    "    outputs = [layers.Dense(dim, activation='softmax')(x) for dim in output_dims]\n",
    "\n",
    "    return models.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Unique output dimensions from the list provided\n",
    "output_dims = [129,129,129,129,129,2,200,32,32,128,64,30,3000,129,5,129]\n",
    "\n",
    "# Create the model\n",
    "model = create_multi_output_model(input_dim=16, embed_dim=64, num_heads=4, ffn_dim=128, num_blocks=2, output_dims=output_dims)\n",
    "model.compile('adam', ['sparse_categorical_crossentropy']*len(output_dims))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-16T18:03:00.467975200Z",
     "start_time": "2023-07-16T18:03:00.178225700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.fit(MIDI_data_extractor('chaconne.mid'),epochs = 2, verbose=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
